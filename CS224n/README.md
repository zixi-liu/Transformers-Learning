
## Table of Contents

### Word Vectors

Build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts, measuring similarity as the vector dot (scalar) product.

- [[Word2Vec] Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
- [[Negative Sampling] Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
- [[NCE] Noise-Contrastive Estimation of Unnormalized Statistical Models with Applications to Natural Image Statistics](https://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf)
- [[GloVe] GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- [On the Dimensionality of Word Embedding](https://proceedings.neurips.cc/paper/2018/file/b534ba68236ba543ae44b22bd110a1d6-Paper.pdf) [[Paper Reading]](https://zhuanlan.zhihu.com/p/355840214)
  - PIP loss, a metric on the dissimilarity between word embeddings
- [Evaluation methods for unsupervised word embeddings](https://aclanthology.org/D15-1036.pdf)
